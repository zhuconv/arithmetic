{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "slurm",
            "type": "debugpy",
            "request": "attach",
            "connect": {
                "host": "127.0.0.1",
                "port": "${config:SOCAT_PORT}"
            },
            "preLaunchTask": "slurm-connect"
        },
        {
            "name": "LongLoRA",
            "type": "debugpy",
            // "request": "launch",
            "request": "attach",
            "connect": {
                "host": "127.0.0.1",
                "port": "${config:SOCAT_PORT}"
            },
            "preLaunchTask": "slurm-debug-connect",
            "program": "${config:python.pythonPath}/../torchrun",
            "args": [
            "--nproc_per_node", "auto",
            "train_longlora.py",
            "--ddp_timeout", "18000",
            "--model_name_or_path", "meta-llama/Llama-2-7b-hf",
            "--peft_type", "ft",  
            "--bf16", "True",
            "--resume_from_checkpoint", "false",
            "--output_dir", "./output/longlora_llama",
            "--model_max_length", "8192",
            "--use_flash_attn", "True",
            "--per_device_train_batch_size", "1",
            "--per_device_eval_batch_size", "2",
            "--gradient_accumulation_steps", "8",
            "--evaluation_strategy", "no",
            "--save_strategy", "steps",
            "--save_steps", "200",
            "--save_total_limit", "2",
            "--learning_rate", "2e-5",
            "--weight_decay", "0.0",
            "--warmup_steps", "20",
            "--lr_scheduler_type", "constant_with_warmup",
            "--logging_steps", "1",
            "--deepspeed", "${workspaceFolder}/config/ds_configs/stage2.json",
            "--tf32", "True",
            "--max_steps", "1000",
            "--report_to", "none"
          ],
            // "env": {
            //     "CUDA_VISIBLE_DEVICES": "6",
            // },
            // "console": "integratedTerminal",
            "justMyCode": false
        },
    ]
}